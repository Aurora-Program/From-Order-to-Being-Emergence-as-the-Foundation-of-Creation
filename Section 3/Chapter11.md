Chapter 11. Understanding Embedding

To understand the reality of artificial intelligence, it is useful to begin with one of its fundamental mechanisms: embedding. This process consists of converting words into numerical vectors through Large Language Models (LLMs).

Before going further, it is important to clarify what is meant by a numerical vector. A vector is, in essence, a representation of reality through numbers. A simple example is the RGB model: to describe a color, one only needs to specify its red, green, and blue components. With just three values, it is possible to represent any color within the spectrum visible to the human eye. Each dimension of the vector reflects a quantifiable characteristic of the color.

In a similar way, LLMs transform words into numerical vectors depending on their context, that is, the surrounding words. At first glance, these tensors might appear to lack semantic meaning, seeming instead to capture only probabilistic distributions. However, thanks to the so-called law of large numbers, such probabilities ultimately reveal consistent patterns in language use.

Several examples illustrate this clearly: if a word frequently appears preceded by the article “la,” it is reasonable to infer that it is a singular noun. If another word often occurs near “astro,” it can be assumed that it belongs to the semantic field of astronomy.

Thus, from purely numerical representations, semantic vectors emerge. Although it is not always possible to determine precisely what each individual dimension means, one can affirm that their combination produces a semantic value representing the word.

As if by a kind of magic—or by a mechanism analogous to the recorder described in earlier chapters—the meaning of the word becomes encoded in a numerical tensor.